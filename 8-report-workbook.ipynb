{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一回レポート課題\n",
    "\n",
    "\n",
    "第一回の課題は，`FrozenLake`をよりよい成績で学習する学習エージェントのクラスを作成することです。提出物等は，以下を通りです。\n",
    "\n",
    "1. **プログラムの提出** : このシートを利用します。\n",
    "2. **レポートの提出**: 詳細はelearningのページを見てください。\n",
    "3. **レポートの相互採点**: 後日演習時に説明します。\n",
    "\n",
    "**[重要]** 第一回レポートは**単位取得の必須条件**です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提出レポートについて\n",
    "\n",
    "\n",
    "### フォーマットについて\n",
    "\n",
    "1. **A3, 1頁(片面)**\n",
    "2. **pdf形式で提出**\n",
    "3. レポートには**学籍番号や氏名はレポート中に記入しないこと**\n",
    "\n",
    "### 書いてはいけないこと\n",
    "\n",
    "1. **`FrozenLake`の環境の概要**(環境やルールの説明)**は書かないこと**\n",
    "2. **`python`のプログラムは書いてはいけない**。**関数名や変数名も書いてはいけない。**\n",
    "\n",
    "\n",
    "### 書くべきこと\n",
    "\n",
    "1. Q学習について(簡潔に)\n",
    "2. 学習エージェントの学習性能の向上のために工夫した点\n",
    "    1. **提案方法:** どんな方法を考えたか。(複数の異なる方法を検討し，検討すること。)\n",
    "    2. **検討手法:** 考えたいくつかの方法のいずれが良いかを判定したデータ\n",
    "    3. **考察:** 良い結果を出した方法が，何故他の方法に良いのか，その理由に関する考察。\n",
    "\n",
    "\n",
    "### 気をつけるべきこと\n",
    "\n",
    "**商品の宣伝のチラシのように見やすいデザイン**にすること。\n",
    "\n",
    "- たくさん書いてあってもわかりにくいのはダメ\n",
    "- 情報が少なすぎるのもダメ。\n",
    "\n",
    "### レポートの採点について\n",
    "\n",
    "- **レボートのデザイン， 内容について以下の点に留意して相互採点**していただきます。\n",
    "- 他者のレポートを採点した内容と、他者からの評価内容の両方が成績に反映します。\n",
    "- 以下で，**単に文章を枠で囲んだだけのものは図とはみなしません**\n",
    "\n",
    "\n",
    "1. **レポートのデザイン**\n",
    "    1. **明瞭性:** 図や文章が用紙内に見やすくなるよう，**レイアウトの工夫**がなされているか\n",
    "\n",
    "2. **図**\n",
    "    1. **明瞭性:** 個々の図は色分けやデザインを工夫して、その**内容が分かりやすい**ものになっているか。また，各図について、**簡潔かつ分かりやすい解説**がなされているか。    \n",
    "    1. **正確性:** グラフであれば，**各軸のタイトルや目盛り**は適切に表示されているか。\n",
    "\n",
    "3. **Q学習の説明**\n",
    "    1. **明瞭性:** Q学習について知らない人にも**わかりやすい説明**になっているか。\n",
    "    2. **正確性:** Q学習についての説明に**間違いはない**か。\n",
    "\n",
    "4. **提案方法**\n",
    "    1. **明瞭性:** 学習性能を上げるために検討した**方法を具体的にわかりやすく説明**しているか。\n",
    "    2. **多様性:** (一つだけではなく)**さまざまな，有効に思われる手法**を考えているか。\n",
    "    3. **独自性:** 提案手法には，**独自の工夫**がなされているか。独創的なものではなくても，**独自の調査**に基づくものか。\n",
    "\n",
    "5. **検討手法**\n",
    "    1. **妥当性:** 考えたいずれの方法がより良いものかを判断するための検討**手法は適切**か。また，検討**結果(主張)は適切**か。\n",
    "    1. **証拠の明示:** 検討結果を示す図等が明示されているか。\n",
    "\n",
    "6. **考察**\n",
    "    1. **根拠の明瞭性:** 検討の結果，良い学習方法と判断された方法が，他の方法に比べて**なぜ良いのか，その理由**が十分に考察され，わかりやすく説明されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLake プログラム提出について\n",
    "\n",
    "`FrozenLake8x8-v0`に対して作成した，最もパフォーマンスの良い学習エージェントプログラムを，以下の指示に従って提出してください。\n",
    "\n",
    "**課題1.** このファイルの名前を以下のように変更しなさい。\n",
    "\n",
    "```\n",
    "FrozenLake-<学籍番号>-<名前>.ipynb\n",
    "例) FrozenLake-16-2202-099-9-YamaguchiDai.ipynb\n",
    "```\n",
    "\n",
    "- 名前は必ず，姓名の順にすること\n",
    "- **ファイル名が不適切な場合は減点**します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 名前の変数定義\n",
    "\n",
    "**課題2** 次の実行セルに，以下のように`id`と`name`を定義してください。\n",
    "\n",
    "- 学籍番号や名前の記載方法は，上記のファイル名と同様の注意を守ること\n",
    "\n",
    "```\n",
    "id=\"学籍番号\"\n",
    "name=\"名前\"\n",
    "```\n",
    "以下は名前が\"山口 大\"の場合の例\n",
    "```\n",
    "id=\"16-2202-099-9\"\n",
    "name=\"YamaguchiDai\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=\"15-2202-047-1\"\n",
    "name=\"HiramatsuMichitaka\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "**課題3**\n",
    "\n",
    "次のセルでは，モジュールや`FrozenLake8x8`の読み込みと，評価に使うためのプログラムの定義をしています。\n",
    "\n",
    "- **randomモジュール**はここで読み込んでいます。学習エージェントのクラス定義に必要な場合は，モジュール名`ra`でアクセスしてください。\n",
    "- **`action_list`の定義**もあります。これも必要に応じてクラス定義に使いましょう。\n",
    "- **この課題の内容**: 次のセルは**Markdownセル**になっています。これを**Codeセルに変更して実行**してください。ただし，**セルの内容を編集してはいけません。編集した場合は不正行為**とみなします。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "action_list=list(range(env.action_space.n))\n",
    "\n",
    "def plot_score(score):\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.ylabel(\"mean return\")\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylim(min(score),max(score))\n",
    "    plt.plot(score)\n",
    "    plt.show()\n",
    "    \n",
    "def run_episodes(env, agent, max_episode=200, learning=True):\n",
    "    history_score=np.zeros(max_episode)\n",
    "    history_step=np.zeros(max_episode)\n",
    "    \n",
    "    for n_episode in range(max_episode):\n",
    "        ra.seed()\n",
    "        score=0\n",
    "        s=env.reset()\n",
    "        for t in range(env.spec.max_episode_steps):\n",
    "            s_old=s\n",
    "            action=agent.get_action(s)\n",
    "            s,r,terminal,_=env.step(action)\n",
    "            score+=r\n",
    "            if learning==True: agent.learning(s_old,action,r,s)\n",
    "            if terminal: break\n",
    "        history_score[n_episode]=score\n",
    "        history_step[n_episode]=t+1\n",
    "\n",
    "    return(history_step,history_score)\n",
    "\n",
    "def analyze_scores(scores,condition):\n",
    "    above_steps=np.where(scores>0.2)\n",
    "    above_duration=len(above_steps[0])\n",
    "    min_above_step=-1 if above_duration==0 else np.nanmin(above_steps)\n",
    "\n",
    "    print(\"[Your Score]\")\n",
    "    print(\"beyond 0.2 at index: \", min_above_step)\n",
    "    print(\"# of beyond 0.2: \", above_duration)\n",
    "\n",
    "    fname=\"FrozenLake-\"+id+\"-\"+name+\".dat\"\n",
    "    with open(fname, mode='w') as f:\n",
    "        f.write(\"{},{},{},{},\".format(id,name,above_duration,min_above_step)+condition)\n",
    "    \n",
    "def mean_performance(env, agent, n_trial=100, max_episode=3000):\n",
    "    filter = np.ones(env.spec.trials)/env.spec.trials\n",
    "    max_episode0=max_episode\n",
    "    max_episode+=env.spec.trials-1\n",
    "    \n",
    "    mean_steps=np.zeros(max_episode)\n",
    "    mean_scores=np.zeros(max_episode)\n",
    "\n",
    "    for trial in range(n_trial):\n",
    "        agent.reset()\n",
    "        steps,scores=run_episodes(env=env, agent=agent, max_episode=max_episode)\n",
    "        mean_steps+=steps\n",
    "        mean_scores+=scores\n",
    "    \n",
    "    mean_steps=np.convolve(mean_steps, filter, mode='valid')\n",
    "    mean_scores=np.convolve(mean_scores, filter, mode='valid')\n",
    "    mean_steps/=(n_trial+1)\n",
    "    mean_scores/=(n_trial+1)\n",
    "    \n",
    "    print(\"# of trials: %d, \" % (n_trial), end='')\n",
    "    print(\"episodes/trial: %d, \" % (max_episode0), end='')\n",
    "    print(\"steps/episode: %d, \" % (env.spec.max_episode_steps), end='')\n",
    "    print(\"window size (steps): %d\" % (env.spec.trials))\n",
    "    condition=\"{}-{}-{}-{}\".format(n_trial,max_episode0,env.spec.max_episode_steps,env.spec.trials)\n",
    "    analyze_scores(mean_scores,condition)\n",
    "\n",
    "    return(mean_steps,mean_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as ra\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "action_list=list(range(env.action_space.n))\n",
    "\n",
    "def plot_score(score):\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"mean return\")\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylim(min(score),max(score))\n",
    "    plt.plot(score)\n",
    "    plt.show()\n",
    "\n",
    "def run_episodes(env, agent, max_episode=200, learning=True):\n",
    "    history_score=np.zeros(max_episode)\n",
    "    history_step=np.zeros(max_episode)\n",
    "\n",
    "    for n_episode in range(max_episode):\n",
    "        ra.seed()\n",
    "        score=0\n",
    "        s=env.reset()\n",
    "        for t in range(env.spec.max_episode_steps):\n",
    "            s_old=s\n",
    "            action=agent.get_action(s)\n",
    "            s,r,terminal,_=env.step(action)\n",
    "            score+=r\n",
    "            if learning==True: agent.learning(s_old,action,r,s)\n",
    "            if terminal: break\n",
    "        history_score[n_episode]=score\n",
    "        history_step[n_episode]=t+1\n",
    "\n",
    "    return(history_step,history_score)\n",
    "\n",
    "def analyze_scores(scores,condition):\n",
    "    above_steps=np.where(scores>0.2)\n",
    "    above_duration=len(above_steps[0])\n",
    "    min_above_step=-1 if above_duration==0 else np.nanmin(above_steps)\n",
    "\n",
    "    print(\"[Your Score]\")\n",
    "    print(\"beyond 0.2 at index: \", min_above_step)\n",
    "    print(\"# of beyond 0.2: \", above_duration)\n",
    "\n",
    "    fname=\"FrozenLake-\"+id+\"-\"+name+\".dat\"\n",
    "    with open(fname, mode='w') as f:\n",
    "        f.write(\"{},{},{},{},\".format(id,name,above_duration,min_above_step)+condition)\n",
    "\n",
    "def mean_performance(env, agent, n_trial=100, max_episode=3000):\n",
    "    filter = np.ones(env.spec.trials)/env.spec.trials\n",
    "    max_episode0=max_episode\n",
    "    max_episode+=env.spec.trials-1\n",
    "\n",
    "    mean_steps=np.zeros(max_episode)\n",
    "    mean_scores=np.zeros(max_episode)\n",
    "\n",
    "    for trial in range(n_trial):\n",
    "        agent.reset()\n",
    "        steps,scores=run_episodes(env=env, agent=agent, max_episode=max_episode)\n",
    "        mean_steps+=steps\n",
    "        mean_scores+=scores\n",
    "\n",
    "    mean_steps=np.convolve(mean_steps, filter, mode='valid')\n",
    "    mean_scores=np.convolve(mean_scores, filter, mode='valid')\n",
    "    mean_steps/=(n_trial+1)\n",
    "    mean_scores/=(n_trial+1)\n",
    "\n",
    "    print(\"# of trials: %d, \" % (n_trial), end='')\n",
    "    print(\"episodes/trial: %d, \" % (max_episode0), end='')\n",
    "    print(\"steps/episode: %d, \" % (env.spec.max_episode_steps), end='')\n",
    "    print(\"window size (steps): %d\" % (env.spec.trials))\n",
    "    condition=\"{}-{}-{}-{}\".format(n_trial,max_episode0,env.spec.max_episode_steps,env.spec.trials)\n",
    "    analyze_scores(mean_scores,condition)\n",
    "\n",
    "    return(mean_steps,mean_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プログラム提出セル\n",
    "**課題4** 次のセルに学習エージェントのクラス定義を記入してください。**クラス名は`Qlearner`としてください。**\n",
    "\n",
    "- 必要に応じて，`Qlearner`が呼び出す関数の定義をしても構いません。\n",
    "- **注意**: Qlearnerは， 課題シート`6-class`にかかれていた仕様を満たしていないと， **性能評価ができない**場合があります。その場合の**評点は0点**になります。気をつけてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Qlearner:\n",
    "    def __init__(self,action_list,epsilon=0.1,alpha=0.2,gamma=0.9,q0=0):\n",
    "            self.action_list=action_list\n",
    "            self.epsilon=epsilon\n",
    "            self.alpha=alpha\n",
    "            self.gamma=gamma\n",
    "            self.q0=q0\n",
    "            self.Q={}\n",
    "    def reset(self):\n",
    "        self.Q={}\n",
    "    \n",
    "    def get_q(self, s, a):\n",
    "        q=self.Q.get((s,a),self.q0)\n",
    "        return(q)\n",
    "        \n",
    "    def get_maxQ(self,s):\n",
    "        q2={}\n",
    "        for i in self.action_list:\n",
    "            q2[(s,i)]=self.get_q(s,i)\n",
    "        max_q2=max(q2.values())\n",
    "        keys=[k[1] for k,v in q2.items() if v==max_q2]\n",
    "        return(max_q2,len(keys),keys)\n",
    "    \n",
    "    def get_action(self,s):\n",
    "        if ra.random()<self.epsilon:\n",
    "            ac=ra.choice(self.action_list)\n",
    "        else:\n",
    "            m,l,k=self.get_maxQ(s)\n",
    "            ac=ra.choice(k)\n",
    "        return(ac)\n",
    "    \n",
    "    def learning(self,s1,a1,r,s2):\n",
    "        m,l,k=self.get_maxQ(s2)\n",
    "        self.Q[s1,a1]=self.get_q(s1,a1)+self.alpha*(r+self.gamma*m-self.get_q(s1,a1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価用セル\n",
    "\n",
    "**課題5.** 以下は提出プログラムの評価のための命令文です。\n",
    "\n",
    "- この命令文を次のセルにコピペして，**実行できることを確認**してください。\n",
    "- **実行時にエラーがでたら採点対象外**になります。\n",
    "- **命令文を変更してはいけません**。\n",
    "\n",
    "**評価プログラムについて**\n",
    "- 以下で呼び出している`mean_performance()`は，学習クラス`Qlearner`の評価を以下のように行う。\n",
    "    - 1回の学習シミュレーション: 「初期状態から3000エピソードの学習」\n",
    "    - 上記学習シミュレーションを100回繰り返し，報酬推移(エピソード終了)の平均を計算\n",
    "    - 100エピソード中の報酬平均が0.2を超えたら，プログラムは合格点(6/10)。成績に応じて追加点。\n",
    "    - 上記成績を得られなかったら，プログラムは0点。\n",
    "    - 実行結果は，ファイル`FrozenLake-<学籍番号>-<名前>.dat`にも書き出される。\n",
    "- `mean_performance()`の実行にはすこし時間がかかるが，引数設定により上記のエピソード回数や平均回数を小さくすれば短縮できる。ただし，**引数を変えて試す場合は，提出用シートでは実行しないこと**。\n",
    "\n",
    "```python\n",
    "%%time\n",
    "q_agent=Qlearner(action_list)\n",
    "steps,scores=mean_performance(env,agent=q_agent)\n",
    "plot_score(scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "q_agent=Qlearner(action_list)\n",
    "steps,scores=mean_performance(env,agent=q_agent)\n",
    "plot_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
